import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):  # 定义Net的一系列属性
        # nn.Module的子类函数必须在构造函数中执行父类的构造函数
        super(Net, self).__init__()  # 等价与nn.Module.__init__()

        self.fc1 = nn.Linear(35, 512)  # 设置第一个全连接层(输入层到隐藏层): 状态数个神经元到50个神经元
        self.fc1.weight.data.normal_(0, 0.1)  # 权重初始化 (均值为0，方差为0.1的正态分布)
        self.fc2 = nn.Linear(512, 256)  # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元
        self.fc2.weight.data.normal_(0, 0.1)  # 权重初始化 (均值为0，方差为0.1的正态分布)
        self.fc3 = nn.Linear(256, 128)  # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元
        self.fc3.weight.data.normal_(0, 0.1)  # 权重初始化 (均值为0，方差为0.1的正态分布)
        self.out = nn.Linear(128, 160)  # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元
        self.out.weight.data.normal_(0, 0.1)  # 权重初始化 (均值为0，方差为0.1的正态分布)

    def forward(self, x):  # 定义forward函数 (x为状态)
        x = F.relu(self.fc1(x))  # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值
        x = F.relu(self.fc2(x))  # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值
        x = F.relu(self.fc3(x))  # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值
        actions_value = self.out(x)  # 连接隐藏层到输出层，获得最终的输出值 (即动作值)
        print(actions_value)
        return actions_value  # 返回动作值

# 创建模型实例
model = Net()

# 创建输入数据
input_data = torch.randn(1, 1, 35)  # 输入数据为1x35

# 前向传播
output = model(input_data)

print(output.size())

'''
tensor([[[ 0.5961, -0.8778, -0.6943, -0.4980,  0.5695, -0.3590,  0.3037,
           0.5871, -0.1727, -1.6833,  0.7744, -0.8796, -1.0010, -1.1004,
           0.4133,  0.0440,  1.0043, -0.7349, -0.3115,  0.4444, -0.5926,
          -1.6019, -0.7477, -0.3728, -0.1534,  0.2989,  0.0177,  0.5794,
          -0.1161, -0.0270, -0.5099,  0.5929,  0.7462,  0.1335,  0.1970,
           1.4382, -0.2604,  1.3851, -0.3993, -1.1331,  1.1789, -1.2450,
          -0.8364, -0.0555,  1.4496, -0.3258, -2.0459,  1.1252,  0.4001,
          -1.0714, -1.0124,  1.0417,  0.2802,  0.7806,  0.2568,  0.1297,
          -0.8725,  0.3995,  0.2462,  1.4570,  0.4567, -0.6282, -1.9972,
           1.2764, -0.1846, -0.5019, -0.2474, -0.9641,  0.3756, -0.4157,
          -1.0293,  0.5565, -0.1875, -0.5153, -1.1523, -0.1066,  0.8601,
          -0.4467,  0.2983, -0.6942,  0.1745, -0.1389,  0.9230, -0.3410,
          -0.5028,  0.3211,  1.4082,  1.0251, -0.2763,  0.2219, -0.0632,
           0.7524, -2.3273, -1.7599,  0.2219, -1.5548,  0.5689,  0.0395,
           0.6006, -0.6264,  0.8322,  1.1765,  1.7620, -0.9794,  1.8153,
           0.9453,  1.4849,  0.2836,  0.1378, -0.5249,  0.7675, -0.1315,
          -0.0989,  1.7896, -0.6383,  1.6437, -1.9115, -0.7680,  0.4740,
           0.2523, -0.6791,  0.5232, -0.7937,  0.3313, -0.9902, -0.6349,
          -0.4500, -0.4127,  1.0493,  0.5010,  1.0788,  0.6070,  0.3130,
           2.3212, -0.3268,  0.4355,  0.5592, -1.0681, -0.7538, -1.3158,
          -0.8007, -0.0729, -1.1530, -1.4353, -0.9654,  0.2848, -0.6454,
           0.1394,  0.9283,  0.8786,  1.3552, -0.8691,  1.1019, -0.8867,
          -0.8704, -0.2587, -0.4590, -0.2767, -0.7449, -0.1906]]],
       grad_fn=<AddBackward0>)
'''
